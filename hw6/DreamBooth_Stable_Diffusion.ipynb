{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"XU7NuMAA2drw","outputId":"fd28788a-b463-4955-b62d-dbb9d50d08e9","executionInfo":{"status":"ok","timestamp":1681214309015,"user_tz":-180,"elapsed":10,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla T4, 15360 MiB, 15101 MiB\n"]}],"source":["#@markdown Check type of GPU and VRAM available.\n","!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"]},{"cell_type":"markdown","metadata":{"id":"BzM7j0ZSc_9c"},"source":["https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"]},{"cell_type":"markdown","metadata":{"id":"wnTMyW41cC1E"},"source":["## Install Requirements"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aLWXPZqjsZVV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681214368299,"user_tz":-180,"elapsed":53557,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}},"outputId":"e7b83159-ee28-4ffb-a4de-299a62dad3ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n","!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n","%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n","%pip install -q -U --pre triton\n","%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","id":"y4lqqWT_uxD2","executionInfo":{"status":"ok","timestamp":1681214493918,"user_tz":-180,"elapsed":970,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"outputs":[],"source":["#@title Login to HuggingFace 🤗\n","\n","#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in 🤗 Hugging Face Hub, and you'll also need to use an access token for the code to work.\n","# https://huggingface.co/settings/tokens\n","!mkdir -p ~/.huggingface\n","HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n","!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"]},{"cell_type":"markdown","metadata":{"id":"G0NV324ZcL9L"},"source":["## Settings and run"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","id":"Rxg0y5MBudmd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681215458901,"user_tz":-180,"elapsed":2730,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}},"outputId":"12060347-a0af-4d42-b214-9add453c80c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[*] Weights will be saved at /content/drive/MyDrive/Repo/GANs/DIFFUSION/weights\n"]}],"source":["#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n","save_to_gdrive = True #@param {type:\"boolean\"}\n","if save_to_gdrive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","#@markdown Name/Path of the initial model.\n","MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n","\n","#@markdown Enter the directory name to save model at.\n","\n","OUTPUT_DIR = \"Repo/GANs/DIFFUSION/weights\" #@param {type:\"string\"}\n","if save_to_gdrive:\n","    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n","else:\n","    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n","\n","print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n","\n","!mkdir -p $OUTPUT_DIR"]},{"cell_type":"markdown","metadata":{"id":"qn5ILIyDJIcX"},"source":["# Start Training\n","\n","Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n","\n","\n","| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n","| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n","| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n","| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n","| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n","| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n","| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n","| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n","| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n","| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n","| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"]},{"cell_type":"markdown","metadata":{"id":"-ioxxvHoicPs"},"source":["Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n","\n","remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n","\n","remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"5vDpCxId1aCm","executionInfo":{"status":"ok","timestamp":1681216182650,"user_tz":-180,"elapsed":578,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"outputs":[],"source":["# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n","\n","concepts_list = [\n","    {\n","        \"instance_prompt\":      \"Daniel Moore\",\n","        \"class_prompt\":         \"guy\",\n","        \"instance_data_dir\":    \"/content/src\",\n","        \"class_data_dir\":       \"/content/src/guy\"\n","    },\n","#     {\n","#         \"instance_prompt\":      \"photo of ukj person\",\n","#         \"class_prompt\":         \"photo of a person\",\n","#         \"instance_data_dir\":    \"/content/data/ukj\",\n","#         \"class_data_dir\":       \"/content/data/person\"\n","#     }\n","]\n","\n","# `class_data_dir` contains regularization images\n","import json\n","import os\n","for c in concepts_list:\n","    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n","\n","with open(\"concepts_list.json\", \"w\") as f:\n","    json.dump(concepts_list, f, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"32gYIDDR1aCp"},"outputs":[],"source":["#@markdown Upload your images by running this cell.\n","\n","#@markdown OR\n","\n","#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster). You can also upload your own class images in `class_data_dir` if u don't wanna generate with SD.\n","\n","import os\n","from google.colab import files\n","import shutil\n","\n","for c in concepts_list:\n","    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n","    uploaded = files.upload()\n","    for filename in uploaded.keys():\n","        dst_path = os.path.join(c['instance_data_dir'], filename)\n","        shutil.move(filename, dst_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jjcSXTp-u-Eg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681217319346,"user_tz":-180,"elapsed":1130676,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}},"outputId":"28e27a06-461d-448a-cf44-96e69ed934c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-11 12:29:54.330433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_hf_folder.py:92: UserWarning: A token has been found in `/root/.huggingface/token`. This is the old path where tokens were stored. The new location is `/root/.cache/huggingface/token` which is configurable using `HF_HOME` environment variable. Your token has been copied to this new location. You can now safely delete the old token file manually or use `huggingface-cli logout`.\n","  warnings.warn(\n","Downloading (…)lve/main/config.json: 100% 547/547 [00:00<00:00, 86.0kB/s]\n","Downloading (…)ch_model.safetensors: 100% 335M/335M [00:01<00:00, 191MB/s]\n","/usr/local/lib/python3.9/dist-packages/safetensors/torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  with safe_open(filename, framework=\"pt\", device=device) as f:\n","/usr/local/lib/python3.9/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","/usr/local/lib/python3.9/dist-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  storage = cls(wrap_storage=untyped_storage)\n","Downloading (…)p16/model_index.json: 100% 543/543 [00:00<00:00, 75.3kB/s]\n","unet/diffusion_pytorch_model.safetensors not found\n","Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n","Downloading pytorch_model.bin:   0% 0.00/608M [00:00<?, ?B/s]\u001b[A\n","\n","Downloading (…)rocessor_config.json: 100% 342/342 [00:00<00:00, 39.3kB/s]\n","\n","\n","Fetching 15 files:   7% 1/15 [00:00<00:02,  4.84it/s]\n","\n","\n","Downloading pytorch_model.bin:   0% 0.00/246M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)_checker/config.json: 100% 4.70k/4.70k [00:00<00:00, 174kB/s]\n","Downloading (…)cheduler_config.json: 100% 307/307 [00:00<00:00, 20.4kB/s]\n","\n","\n","Downloading (…)tokenizer/merges.txt:   0% 0.00/525k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 44.7kB/s]\n","\n","Downloading pytorch_model.bin:   2% 10.5M/608M [00:00<00:09, 63.5MB/s]\u001b[A\n","\n","Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 5.10MB/s]\n","\n","\n","Downloading (…)_encoder/config.json:   0% 0.00/636 [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","Downloading (…)_encoder/config.json: 100% 636/636 [00:00<00:00, 7.32kB/s]\n","\n","Downloading pytorch_model.bin:   3% 21.0M/608M [00:00<00:10, 58.6MB/s]\u001b[A\n","\n","\n","Downloading pytorch_model.bin:   9% 21.0M/246M [00:00<00:03, 63.9MB/s]\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)okenizer_config.json: 100% 822/822 [00:00<00:00, 150kB/s]\n","\n","\n","Downloading (…)tokenizer/vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 16.1MB/s]\n","\n","\n","\n","Downloading pytorch_model.bin:  13% 31.5M/246M [00:00<00:02, 77.0MB/s]\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)cc2/unet/config.json: 100% 806/806 [00:00<00:00, 39.2kB/s]\n","\n","Downloading pytorch_model.bin:   7% 41.9M/608M [00:00<00:06, 81.8MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   1% 10.5M/1.72G [00:00<00:25, 67.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)9cc2/vae/config.json: 100% 609/609 [00:00<00:00, 114kB/s]\n","\n","\n","\n","Downloading pytorch_model.bin:  17% 41.9M/246M [00:00<00:03, 65.1MB/s]\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\n","Downloading pytorch_model.bin:   9% 52.4M/608M [00:00<00:07, 72.2MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   1% 21.0M/1.72G [00:00<00:28, 60.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:   6% 10.5M/167M [00:00<00:02, 74.8MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  26% 62.9M/246M [00:00<00:02, 82.4MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   2% 41.9M/1.72G [00:00<00:23, 72.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  13% 21.0M/167M [00:00<00:02, 68.5MB/s]\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  12% 73.4M/608M [00:01<00:07, 74.3MB/s]\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  30% 73.4M/246M [00:00<00:02, 78.3MB/s]\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  19% 31.5M/167M [00:00<00:01, 75.9MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  34% 83.9M/246M [00:01<00:01, 82.7MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   4% 62.9M/1.72G [00:00<00:18, 88.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  16% 94.4M/608M [00:01<00:06, 85.4MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  25% 41.9M/167M [00:00<00:01, 74.6MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  38% 94.4M/246M [00:01<00:01, 78.7MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   4% 73.4M/1.72G [00:00<00:20, 79.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  17% 105M/608M [00:01<00:06, 79.8MB/s] \u001b[A\n","\n","\n","Downloading pytorch_model.bin:  43% 105M/246M [00:01<00:01, 80.2MB/s] \u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  31% 52.4M/167M [00:00<00:01, 74.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   5% 83.9M/1.72G [00:01<00:20, 78.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  19% 115M/608M [00:01<00:06, 77.6MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  38% 62.9M/167M [00:00<00:01, 77.7MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  47% 115M/246M [00:01<00:01, 78.2MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   5% 94.4M/1.72G [00:01<00:19, 83.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  21% 126M/608M [00:01<00:06, 78.7MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  44% 73.4M/167M [00:00<00:01, 84.2MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  51% 126M/246M [00:01<00:01, 78.5MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   6% 105M/1.72G [00:01<00:20, 79.5MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  22% 136M/608M [00:01<00:05, 80.0MB/s]\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  55% 136M/246M [00:01<00:01, 84.3MB/s]\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  50% 83.9M/167M [00:01<00:01, 77.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   7% 115M/1.72G [00:01<00:20, 77.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  60% 147M/246M [00:01<00:01, 83.3MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  24% 147M/608M [00:01<00:05, 78.4MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  63% 105M/167M [00:01<00:00, 86.5MB/s] \u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   7% 126M/1.72G [00:01<00:20, 77.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  64% 157M/246M [00:02<00:01, 85.8MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  26% 157M/608M [00:02<00:05, 79.0MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  69% 115M/167M [00:01<00:00, 87.8MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  68% 168M/246M [00:02<00:00, 89.2MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  28% 168M/608M [00:02<00:05, 82.6MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   8% 136M/1.72G [00:01<00:19, 80.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  75% 126M/167M [00:01<00:00, 88.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   9% 147M/1.72G [00:01<00:19, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  72% 178M/246M [00:02<00:00, 86.1MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  29% 178M/608M [00:02<00:05, 81.7MB/s]\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  77% 189M/246M [00:02<00:00, 90.7MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:   9% 157M/1.72G [00:01<00:19, 81.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  31% 189M/608M [00:02<00:05, 82.9MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  88% 147M/167M [00:01<00:00, 97.7MB/s]\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  81% 199M/246M [00:02<00:00, 93.2MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  33% 199M/608M [00:02<00:07, 56.0MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin:  94% 157M/167M [00:02<00:00, 64.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  10% 178M/1.72G [00:02<00:24, 63.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  85% 210M/246M [00:02<00:00, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  34% 210M/608M [00:02<00:06, 60.8MB/s]\u001b[A\n","\n","Downloading (…)on_pytorch_model.bin: 100% 167M/167M [00:02<00:00, 75.8MB/s]\n","\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  11% 189M/1.72G [00:02<00:22, 66.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  36% 220M/608M [00:02<00:05, 68.2MB/s]\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  94% 231M/246M [00:02<00:00, 73.4MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  12% 199M/1.72G [00:02<00:24, 63.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","Downloading pytorch_model.bin:  98% 241M/246M [00:03<00:00, 74.6MB/s]\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin: 100% 246M/246M [00:03<00:00, 77.0MB/s]\n","\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  13% 220M/1.72G [00:02<00:17, 84.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  40% 241M/608M [00:03<00:05, 70.9MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  13% 231M/1.72G [00:02<00:17, 86.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  41% 252M/608M [00:03<00:04, 76.2MB/s]\u001b[A\n","Downloading pytorch_model.bin:  45% 273M/608M [00:03<00:03, 84.9MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  15% 252M/1.72G [00:03<00:16, 86.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  48% 294M/608M [00:03<00:03, 102MB/s] \u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  16% 273M/1.72G [00:03<00:14, 103MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  52% 315M/608M [00:03<00:02, 100MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  17% 294M/1.72G [00:03<00:16, 88.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  53% 325M/608M [00:04<00:03, 87.5MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  18% 315M/1.72G [00:03<00:13, 104MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  57% 346M/608M [00:04<00:02, 107MB/s] \u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  20% 336M/1.72G [00:03<00:11, 121MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  60% 367M/608M [00:04<00:02, 108MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  21% 357M/1.72G [00:04<00:12, 110MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  64% 388M/608M [00:04<00:01, 111MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  22% 377M/1.72G [00:04<00:11, 118MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  67% 409M/608M [00:04<00:01, 116MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  23% 398M/1.72G [00:04<00:10, 122MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  71% 430M/608M [00:04<00:01, 118MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  24% 419M/1.72G [00:04<00:10, 123MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  74% 451M/608M [00:05<00:01, 120MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  26% 440M/1.72G [00:04<00:10, 123MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  78% 472M/608M [00:05<00:01, 119MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  27% 461M/1.72G [00:04<00:09, 126MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  81% 493M/608M [00:05<00:00, 119MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  28% 482M/1.72G [00:05<00:10, 121MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  84% 514M/608M [00:05<00:00, 129MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  29% 503M/1.72G [00:05<00:09, 126MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  88% 535M/608M [00:05<00:00, 126MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  30% 524M/1.72G [00:05<00:09, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  91% 556M/608M [00:05<00:00, 129MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  32% 545M/1.72G [00:05<00:09, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  95% 577M/608M [00:06<00:00, 126MB/s]\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  33% 566M/1.72G [00:05<00:08, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","Downloading pytorch_model.bin:  98% 598M/608M [00:06<00:00, 127MB/s]\u001b[A\n","\n","\n","\n","Downloading pytorch_model.bin: 100% 608M/608M [00:06<00:00, 94.9MB/s]\n","Fetching 15 files:  27% 4/15 [00:06<00:19,  1.77s/it]\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  35% 608M/1.72G [00:06<00:08, 128MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  37% 629M/1.72G [00:06<00:07, 140MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  38% 661M/1.72G [00:06<00:06, 159MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  40% 682M/1.72G [00:06<00:06, 168MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  41% 703M/1.72G [00:06<00:05, 176MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  42% 724M/1.72G [00:06<00:05, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  44% 755M/1.72G [00:06<00:04, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  46% 786M/1.72G [00:06<00:04, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  48% 818M/1.72G [00:07<00:04, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  49% 849M/1.72G [00:07<00:04, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  51% 881M/1.72G [00:07<00:04, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  52% 902M/1.72G [00:07<00:04, 192MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  54% 933M/1.72G [00:07<00:03, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  56% 965M/1.72G [00:07<00:03, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  57% 986M/1.72G [00:07<00:03, 190MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  59% 1.01G/1.72G [00:08<00:03, 193MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  60% 1.04G/1.72G [00:08<00:03, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  62% 1.07G/1.72G [00:08<00:03, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  64% 1.10G/1.72G [00:08<00:03, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  66% 1.13G/1.72G [00:08<00:02, 196MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  68% 1.16G/1.72G [00:08<00:02, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  70% 1.20G/1.72G [00:09<00:02, 205MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  71% 1.23G/1.72G [00:09<00:02, 204MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  73% 1.26G/1.72G [00:09<00:02, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  75% 1.29G/1.72G [00:09<00:02, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  76% 1.31G/1.72G [00:09<00:02, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  77% 1.33G/1.72G [00:09<00:01, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  79% 1.36G/1.72G [00:09<00:01, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  81% 1.39G/1.72G [00:10<00:01, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  82% 1.42G/1.72G [00:10<00:01, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  84% 1.44G/1.72G [00:10<00:01, 171MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  85% 1.47G/1.72G [00:10<00:01, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  87% 1.50G/1.72G [00:10<00:01, 180MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  89% 1.53G/1.72G [00:10<00:00, 193MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  91% 1.56G/1.72G [00:10<00:00, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  93% 1.59G/1.72G [00:11<00:00, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  94% 1.61G/1.72G [00:11<00:00, 201MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  95% 1.64G/1.72G [00:11<00:00, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  96% 1.66G/1.72G [00:11<00:00, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin:  98% 1.69G/1.72G [00:11<00:00, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Downloading (…)on_pytorch_model.bin: 100% 1.72G/1.72G [00:11<00:00, 147MB/s]\n","Fetching 15 files: 100% 15/15 [00:12<00:00,  1.22it/s]\n","/usr/local/lib/python3.9/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n","  warnings.warn(\n","You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n","04/11/2023 12:30:25 - INFO - __main__ - Number of class images to sample: 20.\n","Generating class images: 100% 5/5 [01:13<00:00, 14.68s/it]\n","\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n","================================================================================\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n","  warn(\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","  warn(\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n","  warn(\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1fbdvf9x4mv9h --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n","  warn(\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","  warn(\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n","  warn(\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n","CUDA SETUP: Detected CUDA version 118\n","CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n","/usr/local/lib/python3.9/dist-packages/diffusers/configuration_utils.py:203: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n","  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n","Downloading (…)cheduler_config.json: 100% 308/308 [00:00<00:00, 125kB/s]\n","Caching latents: 100% 20/20 [00:04<00:00,  4.13it/s]\n","04/11/2023 12:31:57 - INFO - __main__ - ***** Running training *****\n","04/11/2023 12:31:57 - INFO - __main__ -   Num examples = 20\n","04/11/2023 12:31:57 - INFO - __main__ -   Num batches each epoch = 20\n","04/11/2023 12:31:57 - INFO - __main__ -   Num Epochs = 65\n","04/11/2023 12:31:57 - INFO - __main__ -   Instantaneous batch size per device = 1\n","04/11/2023 12:31:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n","04/11/2023 12:31:57 - INFO - __main__ -   Gradient Accumulation steps = 1\n","04/11/2023 12:31:57 - INFO - __main__ -   Total optimization steps = 1300\n","Steps:   0% 0/1300 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n","Steps: 100% 1300/1300 [15:51<00:00,  1.36it/s, loss=0.326, lr=1e-6]unet/diffusion_pytorch_model.safetensors not found\n","You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n","\n","Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n","Generating samples:  25% 1/4 [00:05<00:16,  5.35s/it]\u001b[A\n","Generating samples:  50% 2/4 [00:09<00:08,  4.46s/it]\u001b[A\n","Generating samples:  75% 3/4 [00:13<00:04,  4.45s/it]\u001b[A\n","Generating samples: 100% 4/4 [00:17<00:00,  4.35s/it]\n","[*] Weights saved at /content/drive/MyDrive/Repo/GANs/DIFFUSION/weights/1300\n","Steps: 100% 1300/1300 [16:32<00:00,  1.31it/s, loss=0.326, lr=1e-6]\n"]}],"source":["!python3 train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --revision=\"fp16\" \\\n","  --with_prior_preservation --prior_loss_weight=1.0 \\\n","  --seed=1337 \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --train_text_encoder \\\n","  --mixed_precision=\"fp16\" \\\n","  --use_8bit_adam \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=1e-6 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --num_class_images=20 \\\n","  --sample_batch_size=4 \\\n","  --max_train_steps=1300 \\\n","  --save_interval=10000 \\\n","  --save_sample_prompt=\"photo of Daniel Moore\" \\\n","  --concepts_list=\"concepts_list.json\"\n","\n","# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n","# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"89Az5NUxOWdy"},"outputs":[],"source":["#@markdown Specify the weights directory to use (leave blank for latest)\n","WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n","if WEIGHTS_DIR == \"\":\n","    from natsort import natsorted\n","    from glob import glob\n","    import os\n","    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n","print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"h7tg1nkhLG9Y"},"outputs":[],"source":["#@markdown Run to generate a grid of preview images from the last saved weights.\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","weights_folder = OUTPUT_DIR\n","folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n","\n","row = len(folders)\n","col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n","scale = 4\n","fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n","\n","for i, folder in enumerate(folders):\n","    folder_path = os.path.join(weights_folder, folder)\n","    image_folder = os.path.join(folder_path, \"samples\")\n","    images = [f for f in os.listdir(image_folder)]\n","    for j, image in enumerate(images):\n","        if row == 1:\n","            currAxes = axes[j]\n","        else:\n","            currAxes = axes[i, j]\n","        if i == 0:\n","            currAxes.set_title(f\"Image {j}\")\n","        if j == 0:\n","            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n","        image_path = os.path.join(image_folder, image)\n","        img = mpimg.imread(image_path)\n","        currAxes.imshow(img, cmap='gray')\n","        currAxes.axis('off')\n","        \n","plt.tight_layout()\n","plt.savefig('grid.png', dpi=72)"]},{"cell_type":"markdown","metadata":{"id":"5V8wgU0HN-Kq"},"source":["## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dcXzsUyG1aCy"},"outputs":[],"source":["#@markdown Run conversion.\n","ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n","\n","half_arg = \"\"\n","#@markdown  Whether to convert to fp16, takes half the space (2GB).\n","fp16 = True #@param {type: \"boolean\"}\n","if fp16:\n","    half_arg = \"--half\"\n","!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n","print(f\"[*] Converted ckpt saved at {ckpt_path}\")"]},{"cell_type":"markdown","metadata":{"id":"ToNG4fd_dTbF"},"source":["## Inference"]},{"cell_type":"code","source":["WEIGHTS_DIR = '/content/drive/MyDrive/Repo/GANs/DIFFUSION/weights/1300'"],"metadata":{"id":"gKGUfRjfXQI-","executionInfo":{"status":"ok","timestamp":1681217386225,"user_tz":-180,"elapsed":233,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"gW15FjffdTID","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681217423221,"user_tz":-180,"elapsed":34799,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}},"outputId":"45f9b7e5-22ec-445a-e91d-00e6234fbf1e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n","  warnings.warn(\n","You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"]}],"source":["import torch\n","from torch import autocast\n","from diffusers import StableDiffusionPipeline, DDIMScheduler\n","from IPython.display import display\n","\n","model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n","\n","pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n","pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n","pipe.enable_xformers_memory_efficient_attention()\n","g_cuda = None"]},{"cell_type":"code","execution_count":12,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"oIzkltjpVO_f","outputId":"ab85ae9c-d2b2-4084-8ec1-610e1d9416c1","executionInfo":{"status":"ok","timestamp":1681217494638,"user_tz":-180,"elapsed":251,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f9706380130>"]},"metadata":{},"execution_count":12}],"source":["#@markdown Can set random seed here for reproducibility.\n","g_cuda = torch.Generator(device='cuda')\n","seed = 52362 #@param {type:\"number\"}\n","g_cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":236,"metadata":{"colab":{"referenced_widgets":["a7a9e1991bed496699f5a3391c065a13","0d5637aac9b5478c855db3591b8d3e25","8c42e0597dce49e995cc3b1b7163d92c","0ad42faa3d574cacaff045bd9f3b142b","803a2f7bf65b4efd86cfee405ab80628","079ae213f88c475a87301ee93f1db219","81fe7a82836847f9814d653c9561f6ae","8cc89b93217c4165960614bad9a469d7","c6a2d96b16db4c608c83b1e126720a6d","4050a92962ec42dab901c80411a5d630","10d7e172160f4095a4f5528a1cde13d0"],"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1c-221GBFQ-Jh1PV6Yg-bga17KpmkeOMQ"},"id":"K6xoHWSsbcS3","outputId":"cb97a975-2609-45a9-8791-b2ca21d2785a","scrolled":false,"executionInfo":{"status":"ok","timestamp":1681231592362,"user_tz":-180,"elapsed":30956,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Run for generating images\n","\n","prompt = \"Daniel Moore wearing cyberpunk intricate streetwear, respirator, beautiful, detailed portrait, cell shaded, 4 k, vivid colours, concept art, by wlop, ilya kuvshinov, artgerm, krenz cushart, greg rutkowski, pixiv. cinematic dramatic atmosphere, sharp focus, volumetric lighting, cinematic lighting, studio quality\" \n","negative_prompt = \"\" \n","num_samples = 4 \n","guidance_scale = 4\n","num_inference_steps = 50\n","height = 512 \n","width = 512 \n","\n","with autocast(\"cuda\"), torch.inference_mode():\n","    images = pipe(\n","        prompt,\n","        height=height,\n","        width=width,\n","        negative_prompt=negative_prompt,\n","        num_images_per_prompt=num_samples,\n","        num_inference_steps=num_inference_steps,\n","        guidance_scale=guidance_scale,\n","        generator=g_cuda\n","    ).images\n","\n","for img in images:\n","    display(img)"]},{"cell_type":"code","source":["for idx, image in enumerate(images):\n","    image.save(f'/content/inference/{idx}.png')"],"metadata":{"id":"6uy0l8e5cWr-","executionInfo":{"status":"ok","timestamp":1681232060022,"user_tz":-180,"elapsed":643,"user":{"displayName":"Daniel Artamonov","userId":"17966569171569158391"}}},"execution_count":237,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WMCqQ5Tcdsm2"},"outputs":[],"source":["#@markdown Run Gradio UI for generating images.\n","import gradio as gr\n","\n","def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n","    with torch.autocast(\"cuda\"), torch.inference_mode():\n","        return pipe(\n","                prompt, height=int(height), width=int(width),\n","                negative_prompt=negative_prompt,\n","                num_images_per_prompt=int(num_samples),\n","                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n","                generator=g_cuda\n","            ).images\n","\n","with gr.Blocks() as demo:\n","    with gr.Row():\n","        with gr.Column():\n","            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n","            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n","            run = gr.Button(value=\"Generate\")\n","            with gr.Row():\n","                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n","                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n","            with gr.Row():\n","                height = gr.Number(label=\"Height\", value=512)\n","                width = gr.Number(label=\"Width\", value=512)\n","            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n","        with gr.Column():\n","            gallery = gr.Gallery()\n","\n","    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n","\n","demo.launch(debug=True)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"a7a9e1991bed496699f5a3391c065a13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d5637aac9b5478c855db3591b8d3e25","IPY_MODEL_8c42e0597dce49e995cc3b1b7163d92c","IPY_MODEL_0ad42faa3d574cacaff045bd9f3b142b"],"layout":"IPY_MODEL_803a2f7bf65b4efd86cfee405ab80628"}},"0d5637aac9b5478c855db3591b8d3e25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_079ae213f88c475a87301ee93f1db219","placeholder":"​","style":"IPY_MODEL_81fe7a82836847f9814d653c9561f6ae","value":"100%"}},"8c42e0597dce49e995cc3b1b7163d92c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cc89b93217c4165960614bad9a469d7","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6a2d96b16db4c608c83b1e126720a6d","value":50}},"0ad42faa3d574cacaff045bd9f3b142b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4050a92962ec42dab901c80411a5d630","placeholder":"​","style":"IPY_MODEL_10d7e172160f4095a4f5528a1cde13d0","value":" 50/50 [00:27&lt;00:00,  1.76it/s]"}},"803a2f7bf65b4efd86cfee405ab80628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"079ae213f88c475a87301ee93f1db219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81fe7a82836847f9814d653c9561f6ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cc89b93217c4165960614bad9a469d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6a2d96b16db4c608c83b1e126720a6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4050a92962ec42dab901c80411a5d630":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10d7e172160f4095a4f5528a1cde13d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}